## ğŸ¤– AI Engineer in the Making

![Typing](https://readme-typing-svg.herokuapp.com?size=22&color=00F5FF&lines=Computer+Engineer;Diving+Deep+into+AI;ML+%7C+DL+%7C+NLP+%7C+Transformers;Building+Intelligent+Systems)

---

## ğŸ§  About Me

I am a **Computer Engineer** diving deep into **Artificial Intelligence**, exploring how machines learn, reason, and understand language.

My focus spans **Machine Learning, Deep Learning, Natural Language Processing, Transformers, and Large Language Models**. I prioritize strong fundamentals, hands-on implementation, and understanding model internals rather than treating AI as a black box.

I work primarily in **Python**, experimenting with modern frameworks while continuously strengthening my theoretical foundations.

---

## ğŸ§¬ AI Skills Matrix

### ğŸ Programming
![Python](https://img.shields.io/badge/Python-00f5ff?style=for-the-badge&logo=python)

### ğŸ§  Machine Learning
![ML](https://img.shields.io/badge/Supervised_Learning-0f2027?style=for-the-badge)
![ML](https://img.shields.io/badge/Unsupervised_Learning-2c5364?style=for-the-badge)
![ML](https://img.shields.io/badge/Model_Evaluation-000000?style=for-the-badge)

### ğŸ”¥ Deep Learning
![DL](https://img.shields.io/badge/Neural_Networks-1e1e1e?style=for-the-badge)
![DL](https://img.shields.io/badge/CNNs-2c5364?style=for-the-badge)
![DL](https://img.shields.io/badge/RNNs-0f2027?style=for-the-badge)
![DL](https://img.shields.io/badge/Backpropagation-000000?style=for-the-badge)

### ğŸ—£ï¸ NLP
![NLP](https://img.shields.io/badge/Text_Preprocessing-2c5364?style=for-the-badge)
![NLP](https://img.shields.io/badge/Tokenization-0f2027?style=for-the-badge)
![NLP](https://img.shields.io/badge/Embeddings-000000?style=for-the-badge)

### ğŸ”— Transformers & LLMs
![Transformers](https://img.shields.io/badge/Attention_Mechanism-ff6f00?style=for-the-badge)
![Transformers](https://img.shields.io/badge/Encoderâ€“Decoder-2c5364?style=for-the-badge)
![Transformers](https://img.shields.io/badge/LLMs-1e1e1e?style=for-the-badge)
![Transformers](https://img.shields.io/badge/Prompt_Engineering-0f2027?style=for-the-badge)

---

## ğŸ› ï¸ Tools & Frameworks
![NumPy](https://img.shields.io/badge/NumPy-000000?style=for-the-badge)
![Pandas](https://img.shields.io/badge/Pandas-2c5364?style=for-the-badge)
![Scikit](https://img.shields.io/badge/Scikit_Learn-ff6f00?style=for-the-badge)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge)
![HuggingFace](https://img.shields.io/badge/Hugging_Face-FFD21E?style=for-the-badge)

---

## âš™ï¸ Currently Working On
- ğŸ§  Strengthening ML & DL fundamentals  
- ğŸ—£ï¸ Building NLP pipelines  
- ğŸ”— Understanding Transformer internals  
- ğŸ¤– Exploring Large Language Models  
- ğŸ§ª Python-based AI experiments  

---

## ğŸ§ª AI Knowledge Vault (Click to Expand)
<details>
<summary>ğŸ“‚ Open AI Vault</summary>

- Why attention works better than recurrence  
- How embeddings capture semantic meaning  
- Training vs fine-tuning vs inference  
- Scaling laws of Large Language Models  
- Limitations & hallucinations in LLMs  

</details>

---

## ğŸŒŒ AI Mindset
> "AI is not magic â€” it is mathematics, data, and engineering at scale."

---

<!-- Easter Egg -->
<!-- Attention is all you need -->
